{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVGI6gHpMuwG"
      },
      "source": [
        "# **FIAP Pós-Tech - IA para Devs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHuEOWXtM0Ru"
      },
      "source": [
        "## TURMA 1IADT - GRUPO 14 - ALUNOS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZnzCsfXM3LL"
      },
      "source": [
        "* Marcelo Henriques da Fonseca - RM353865\n",
        "* Marcos Lopes da Silva Junior - RM353763\n",
        "* Ricardo Báfica Pontes - RM353866"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TECH CHALLENGE - FASE 4"
      ],
      "metadata": {
        "id": "N12udYXdGh76"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdB2vpxhNAp8"
      },
      "source": [
        "### O PROBLEMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBeHMhcJNG1T"
      },
      "source": [
        "O Tech Challenge desta fase será a criação de uma aplicação que utilize análise de vídeo. O seu projeto deve incorporar as técnicas de reconhecimento facial, análise de expressões emocionais em vídeos e detecção de atividades."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PARvUghINLsl"
      },
      "source": [
        "### A PROPOSTA DO DESAFIO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efoxCKisNONZ"
      },
      "source": [
        "Você deverá criar uma aplicação a partir do vídeo que se encontra disponível na plataforma do aluno, e que execute as seguintes tarefas:\n",
        "1. **Reconhecimento facial:** Identifique e marque os rostos presentes no vídeo.\n",
        "2. **Análise de expressões emocionais:** Analise as expressões emocionais dos rostos identificados.\n",
        "3. **Detecção de atividades:** Detecte e categorize as atividades sendo realizadas no vídeo.\n",
        "4. **Geração de resumo:** Crie um resumo automático das principais atividades e emoções detectadas no vídeo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMHrMBGnNSUP"
      },
      "source": [
        "### O QUE ESPERAMOS COMO ENTREGÁVEL?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f5DFTGCNU8D"
      },
      "source": [
        "1. **Código Fonte:** todo o código fonte da aplicação deve ser entregue em um repositório Git, incluindo um arquivo README com instruções claras de como executar o projeto.  \n",
        "\n",
        "2. **Relatório:** o resumo obtido automaticamente com as principais atividades e emoções detectadas no vídeo. Nesse momento esperando que o relatório inclua:\n",
        ". Total de frames analisados.\n",
        ". Número de anomalias detectadas.\n",
        "\n",
        "    Observação: movimento anômalo não segue o padrão geral de atividades (como gestos bruscos ou comportamentos atípicos) esses são classificados como anômalos.\n",
        "\n",
        "3. **Demonstração em Vídeo:** um vídeo demonstrando a aplicação em funcionamento, evidenciando cada uma das funcionalidades implementadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPeP2qR-Neit"
      },
      "source": [
        "## PROJETO PROPOSTO PELOS ALUNOS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O projeto dos alunos do grupo 14 propõe a deteção de 8 emoções diferentes e 2 atividades executadas pelas pessoas, que serão reportadas não só sobre o video original, mas também registradas em 3 documentos complementares."
      ],
      "metadata": {
        "id": "JO-n5oG9OXbo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DS5FzysNvrw"
      },
      "source": [
        "### AS EMOÇÕES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okmczE5-tbeD"
      },
      "source": [
        "Detectaremos as seguintes emoções/expressões faciais presentes no vídeo:\n",
        "\n",
        "*   \"Triste\"\n",
        "*   \"Bravo\"\n",
        "*   \"Surpreso\"\n",
        "*   \"Medo\"\n",
        "*   \"Feliz\"\n",
        "*   \"Nojo\"\n",
        "*   \"Neutro\"\n",
        "*   \"Careta\"\n",
        "\n",
        "Das emoções citadas, \"Careta\" configurará uma anomalia a ser detectada.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFSllwzsuY45"
      },
      "source": [
        "### AS ATIVIDADES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fiu3GFVIudy5"
      },
      "source": [
        "Detectaremos as seguintes atividades presentes no vídeo:\n",
        "\n",
        "*   \"Mão levantada\"\n",
        "*   \"Mão no rosto\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie94Cd9nupgy"
      },
      "source": [
        "### OS ENTREGÁVEIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB9QwRfMusJF"
      },
      "source": [
        "Código em python que trata o vídeo oferecido pela FIAP e gera os seguintes entregáveis:\n",
        "\n",
        "*   Vídeo \"output\" baseado no vídeo original, mostrando rostos detectados, emoções detectadas e atividades detectadas\n",
        "*   Arquivo com histórico **completo** das emoções/atividades detectadas no vídeo, registrando todas as emoções/atividades detectadas no vídeo\n",
        "*   Arquivo com histórico **parcial** das emoções/atividades detectadas no vídeo, registrando apenas as emoções/atividades detectadas no frame mediano de cada segundo\n",
        "*   Relatório resumindo as detecções realizadas no vídeo, incluindo o número total de frames analisados e o total de anomalias (caretas) detectadas\n",
        "\n",
        "Este projeto propõe a captura de um único frame específico entre os 30 registrados a cada segundo de vídeo para a criação de um histórico parcial que seria de leitura mais fácil para um ser humano. A percepção humana de movimento contínuo ocorre a partir de 24 frames por segundo, tendo sido este o padrão básico estabelecido pela indústria cinematográfica para seus filmes. No entanto, as emoções faciais humanas tendem a permanecer estáveis por períodos superiores a 1 segundo. Por isso, acreditamos que essa abordagem pode ser eficaz para identificar mudanças significativas nas emoções, sem comprometer a precisão das informações analisadas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT4YbKWeN_n6"
      },
      "source": [
        "## CÓDIGO EM PYTHON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBZod2h1PiOG"
      },
      "source": [
        "Começaremos configurando o acesso ao Google Drive, que é de onde importaremos os arquivos que serão necessários para a análise e também é onde salvaremos os \"outputs\"."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Acesso ao Google Drive"
      ],
      "metadata": {
        "id": "8UoSGd-kB9Py"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELcNW6WACxfL",
        "outputId": "cbdaf92e-24c6-4ceb-ebc8-3ad55d8d2737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Configura acesso ao Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3Hg-D9WPjYv"
      },
      "source": [
        "Nesse bloco definimos os caminhos para os arquivos que serão acessados/criados no Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qX8ivw7hDonp"
      },
      "outputs": [],
      "source": [
        "# Caminho para os arquivos que serão acessados/criados\n",
        "\n",
        "video_path          = \"/content/drive/MyDrive/FIAP/IA_para_Devs/Tech_Challenge_4/Unlocking Facial Recognition_ Diverse Activities Analysis.mp4\"\n",
        "output_path         = \"/content/drive/MyDrive/FIAP/IA_para_Devs/Tech_Challenge_4/output.mp4\"\n",
        "hist_parcial_path   = \"/content/drive/MyDrive/FIAP/IA_para_Devs/Tech_Challenge_4/historico_parcial.csv\"\n",
        "hist_completo_path  = \"/content/drive/MyDrive/FIAP/IA_para_Devs/Tech_Challenge_4/historico_completo.csv\"\n",
        "relat_path          = \"/content/drive/MyDrive/FIAP/IA_para_Devs/Tech_Challenge_4/relatorio.txt\"\n",
        "model_path          = \"/content/drive/MyDrive/FIAP/IA_para_Devs/Tech_Challenge_4/pose_landmarker_heavy.task\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalação e importação de bibliotecas"
      ],
      "metadata": {
        "id": "5aIKh2vuBph9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipkqhIGwxm2Z"
      },
      "source": [
        "Também faremos a instalação e importação de dependências necessárias para a realização do projeto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jxqePW2WODWy"
      },
      "outputs": [],
      "source": [
        "# Instalação da biblioteca DeepFace para análise de expressões emocionais\n",
        "!pip install deepface -q\n",
        "\n",
        "# Instalação da biblioteca Ultralytics (YOLOv8) para detecção de atividades no vídeo\n",
        "!pip install ultralytics==8.3.13 -q\n",
        "\n",
        "# Instalação da biblioteca MediaPipe para reconhecimento facial\n",
        "!pip install mediapipe==0.10.18 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jfmhnu9eCBZP"
      },
      "outputs": [],
      "source": [
        "# Importação da biblioteca OpenCV para processamento de vídeo e imagens\n",
        "import cv2\n",
        "\n",
        "# Importação da biblioteca NumPy para manipulação de arrays e operações matemáticas\n",
        "import numpy as np\n",
        "\n",
        "# Importação do tqdm para criar barras de progresso\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Importação do DeepFace para análise de expressões emocionais\n",
        "from deepface import DeepFace\n",
        "\n",
        "# Importação da biblioteca Ultralytics (YOLOv8) para detecção de atividades\n",
        "import ultralytics\n",
        "\n",
        "# Importação da biblioteca MediaPipe para reconhecimento facial\n",
        "import mediapipe as mp\n",
        "\n",
        "# Importação da biblioteca CSV para manipulação de arquivos CSV\n",
        "import csv\n",
        "\n",
        "# Importação do PrettyTable para criação de tabelas com visualização amigável\n",
        "from prettytable import PrettyTable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uso do MediaPipe e configuração do PoseLandmarker"
      ],
      "metadata": {
        "id": "-jo69XOFClOW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuOlSCW7Pzzm"
      },
      "source": [
        "Neste momento são estabelecidas as opções que serão utilizadas pela biblioteca mediapipe (abreviada como mp) para configurar e inicializar um sistema de detecção de poses humanas em imagens.  \n",
        "\n",
        "- Definimos a confiança mínima para detectar a presença de uma pose em 0.8, o que indica que o algoritmo precisa de uma alta confiança (80%) para considerar que uma pose está presente.  \n",
        "\n",
        "- Definimos também a confiança mínima para o rastreamento de poses em 0.8, indicando o alto nível de confiança exigido para acompanhar a posição dos pontos detectados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_95jMkuACcTO"
      },
      "outputs": [],
      "source": [
        "# Instanciando as opções básicas para configurar o modelo do MediaPipe\n",
        "BaseOptions = mp.tasks.BaseOptions\n",
        "\n",
        "# Instanciando a classe PoseLandmarker para detecção de poses corporais no MediaPipe\n",
        "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
        "\n",
        "# Instanciando as opções para configuração do PoseLandmarker\n",
        "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
        "\n",
        "# Instanciando o modo de execução do modelo para visão (imagem ou vídeo)\n",
        "VisionRunningMode = mp.tasks.vision.RunningMode\n",
        "\n",
        "# Configuração das opções do modelo de detecção de poses corporais\n",
        "options = PoseLandmarkerOptions(\n",
        "                base_options                    = BaseOptions(model_asset_path=model_path), # Caminho para o modelo\n",
        "                running_mode                    = VisionRunningMode.IMAGE,                  # Modo de execução definido como imagem\n",
        "                min_pose_presence_confidence    = 0.8,                                      # Confiança mínima para presença de pose\n",
        "                min_tracking_confidence         = 0.8,                                      # Confiança mínima para rastreamento\n",
        "            )\n",
        "\n",
        "# Instanciando o objeto PoseLandmarker a partir das opções configuradas\n",
        "landmarker = PoseLandmarker.create_from_options(options)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uso do OpenCV para captura do vídeo"
      ],
      "metadata": {
        "id": "8jd2RV-FEPRV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgPHoZVmP5h3"
      },
      "source": [
        "Aqui é inicializada a captura do vídeo selecionado e são coletadas informações gerais do mesmo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vhy7nTgGDKJ7",
        "outputId": "f81c656c-31b0-45e0-e480-b83c4a3236ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de frames do vídeo original: 3326\n",
            "FPS do vídeo original: 30\n"
          ]
        }
      ],
      "source": [
        "# Carrega o vídeo e extrai alguns parâmetros\n",
        "cap = cv2.VideoCapture(video_path)  # Abre o vídeo especificado pelo caminho\n",
        "\n",
        "# Extrai a taxa de quadros (frames por segundo) do vídeo\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Extrai a largura dos frames do vídeo\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "\n",
        "# Extrai a altura dos frames do vídeo\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Extrai o número total de frames no vídeo\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# Exibe informações sobre o vídeo original\n",
        "print(f\"Total de frames do vídeo original: {total_frames}\")\n",
        "print(f\"FPS do vídeo original: {fps}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOSgfKi6P6wv"
      },
      "source": [
        "Inicializamos o vídeo de saída onde serão \"escritas\" as emoções e atividades citadas anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Zi2536LUDR-e"
      },
      "outputs": [],
      "source": [
        "# Configuração do vídeo de \"saída\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Define o codec de vídeo (usando \"mp4v\" para gerar um arquivo MP4)\n",
        "\n",
        "# Cria um objeto para gravar o vídeo de saída com os parâmetros especificados\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))  # Define o caminho de saída, codec, FPS e resolução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTDe5XmOP8sX"
      },
      "source": [
        "Criamos duas variáveis que servirão para gravar as atividades/emoções que serão detectadas no vídeo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "33-osfBDDiRi"
      },
      "outputs": [],
      "source": [
        "# Inicializa listas para armazenar as atividades e emoções detectadas no vídeo\n",
        "historico_parcial  = []\n",
        "historico_completo = []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uso do FaceMesh"
      ],
      "metadata": {
        "id": "OLRS6Ut_EXwB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojYxp6XUP-Cf"
      },
      "source": [
        "Neste bloco começamos inicializando o FaceMesh e o DeepFace que serão necessários para a detectação de expressões faciais.  \n",
        "O FaceMesh é uma solução para detecção de pontos faciais em 3D.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sTxsQLtKP7vd"
      },
      "outputs": [],
      "source": [
        "# Inicialização do FaceMesh para detecção de pontos faciais em 3D.\n",
        "# É importante instanciar este modelo uma vez antes de usá-lo no processamento de vídeo,\n",
        "# porque instanciar o modelo repetidamente pode causar lentidão.\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "# A instância `face_mesh` é configurada com os parâmetros:\n",
        "face_mesh = mp_face_mesh.FaceMesh(\n",
        "    static_image_mode=False,        # - modo de imagem estática desativado, ie, o modelo irá operar em tempo real (modo dinâmico).\n",
        "    max_num_faces=5,                # - número máximo de rostos a serem detectados em cada quadro de vídeo (pode ser ajustado conforme a necessidade).\n",
        "    min_detection_confidence=0.5    # - nível mínimo de confiança para considerar uma detecção como válida (quanto maior, mais preciso).\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo2_-TTFWDbb"
      },
      "source": [
        "Também criamos alguns contadores que nos ajudarão a saber em quantos frames cada emoção aparece e que serão relevantes na hora de gerar o relatório."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UM8Xn9MQV75E"
      },
      "outputs": [],
      "source": [
        "# Criamos contadores de emoções para registrar a quantidade de cada tipo de emoção detectada no vídeo.\n",
        "# O dicionário `cont_emocoes` armazena as contagens de diferentes emoções, que serão incrementadas à medida que forem detectadas.\n",
        "# Estes valores serão relevantes na momento de se gerar o relatório.\n",
        "cont_emocoes = {\n",
        "    \"cont_triste\"   : 0,    # Contador de expressões faciais de tristeza\n",
        "    \"cont_bravo\"    : 0,    # Contador de expressões faciais de raiva\n",
        "    \"cont_surpreso\" : 0,    # Contador de expressões faciais de surpresa\n",
        "    \"cont_medo\"     : 0,    # Contador de expressões faciais de medo\n",
        "    \"cont_feliz\"    : 0,    # Contador de expressões faciais de felicidade\n",
        "    \"cont_nojo\"     : 0,    # Contador de expressões faciais de nojo\n",
        "    \"cont_neutro\"   : 0,    # Contador de expressões faciais neutras (sem emoção clara)\n",
        "    \"cont_careta\"   : 0     # Contador de expressões faciais de careta (sem emoção específica ou expressões extremas)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uPUK8ij1Ux2"
      },
      "source": [
        "É importante destacar que para a análise do DeepFace selecionamos o \"detector_backend\" como \"yolov8\".\n",
        "\n",
        "Diferentes \"detector_backend\" apresentam performances e eficiências diferentes. Alguns detectam melhor os rostos, outros demoram menos tempo.\n",
        "\n",
        "Após diversos testes com os diferentes \"detector_backend\" disponíveis, optamos por utilizar o \"yolov8\" pois foi o que nos gerou os melhores resultados em um tempo de processamento de vídeo razoável. Julgamos que ele seria o mais adequado para uma atividade acadêmica como esta."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uso do DeepFace para detectar rostos e expressões"
      ],
      "metadata": {
        "id": "STpvkJ1KFRlR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "180GGNZ1kqOw"
      },
      "source": [
        "Função detecta_rostos identifica os rostos e suas expressões.  \n",
        "\n",
        "1. Conversão para RGB:  \n",
        "O frame capturado é convertido de BGR para RGB, pois o MediaPipe e o DeepFace funcionam melhor com o formato RGB.\n",
        "\n",
        "2. Detecção de Rostos com DeepFace:  \n",
        "O modelo DeepFace é usado para detectar emoções faciais. O parâmetro detector_backend=\"yolov8\" é escolhido para fornecer um bom equilíbrio entre precisão e desempenho. Ele é ideal para esta aplicação.\n",
        "\n",
        "3. Análise de Emoções:  \n",
        "A emoção predominante é extraída para cada rosto detectado. A tradução das emoções para o português torna a interpretação mais fácil (por exemplo, \"happy\" é traduzido para \"feliz\").\n",
        "\n",
        "4. Detecção de Caretas com MediaPipe:  \n",
        "Usamos o FaceMesh do MediaPipe para mapear pontos específicos da face (boca, olhos, etc.), a partir dos quais calculamos parâmetros como a abertura da boca e dos olhos, a proporção da boca em relação à face e a assimetria da boca.  \n",
        "Se esses parâmetros indicarem uma careta, a confiança em careta é aumentada e a expressão é alterada para \"careta\".\n",
        "\n",
        "5. Contagem das Emoções:  \n",
        "O código conta quantas vezes cada emoção é detectada, armazenando esses dados em um dicionário cont_emocoes.\n",
        "\n",
        "6. Desenhando a Detecção na Tela:  \n",
        "Se a emoção for uma careta, um retângulo vermelho e o texto são desenhados em torno do rosto. Para outras emoções, a cor do retângulo e do texto será verde.\n",
        "\n",
        "7. Gravação do Histórico:  \n",
        "Ao final, a emoção detectada, o número do frame e o tempo são registrados no histórico, tanto completo quanto parcial (se solicitado). Isso é útil para análise posterior ou para visualizar as emoções detectadas ao longo do tempo.\n",
        "\n",
        "8. Comentários Adicionais:  \n",
        "É importante destacar que para a análise do DeepFace selecionamos o \"detector_backend\" como \"yolov8\".  \n",
        "Diferentes \"detector_backend\" apresentam performances e eficiências diferentes. Alguns detectam melhor os rostos, outros demoram menos tempo.  \n",
        "Após diversos testes com os diferentes \"detector_backend\" disponíveis, optamos por utilizar o \"yolov8\" pois foi o que nos gerou os melhores resultados em um tempo de processamento de vídeo razoável."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1O6pj-zkGdeX"
      },
      "outputs": [],
      "source": [
        "def detecta_rostos(frame):\n",
        "    # Define cont_emocoes como variável global\n",
        "    global cont_emocoes\n",
        "\n",
        "    # Converte o frame da imagem para RGB, pois o MediaPipe trabalha melhor com imagens no formato RGB\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Processa o frame usando o FaceMesh do MediaPipe para detectar os pontos faciais\n",
        "    results = face_mesh.process(rgb_frame)\n",
        "\n",
        "    # Vamos usaro DeepFace para detectar emoções/expressões faciais.\n",
        "    rostos = DeepFace.analyze(\n",
        "        frame,\n",
        "        actions=[\"emotion\"],        # especifica que se deseja analisar a emoção da face detectada\n",
        "        enforce_detection=False,    # garante que a detecção facial não precisa ser perfeita\n",
        "        detector_backend=\"yolov8\",  # define o modelo que será usado para detectar os rostos (YoloV8 neste caso)\n",
        "        align=True,\n",
        "    )\n",
        "\n",
        "    # Analise de emoções para cada rosto detectado pelo DeepFace\n",
        "    for rosto in rostos:\n",
        "        confianca = rosto[\"face_confidence\"]    # grau de confiança de detecção do rosto pelo DeepFace\n",
        "        if confianca < 0.7:\n",
        "            continue\n",
        "\n",
        "        # Extraímos as coordenadas do rosto detectado na imagem\n",
        "        x, y, w, h = (\n",
        "            rosto[\"region\"][\"x\"],\n",
        "            rosto[\"region\"][\"y\"],\n",
        "            rosto[\"region\"][\"w\"],\n",
        "            rosto[\"region\"][\"h\"],\n",
        "        )\n",
        "\n",
        "        # Coletamos a emoção dominante identificada pelo DeepFace\n",
        "        emocao = rosto[\"dominant_emotion\"]\n",
        "        if emocao == \"sad\":\n",
        "            emocao = \"triste\"\n",
        "        elif emocao == \"angry\":\n",
        "            emocao = \"bravo\"\n",
        "        elif emocao == \"surprise\":\n",
        "            emocao = \"surpreso\"\n",
        "        elif emocao == \"fear\":\n",
        "            emocao = \"medo\"\n",
        "        elif emocao == \"happy\":\n",
        "            emocao = \"feliz\"\n",
        "        elif emocao == \"disgust\":\n",
        "            emocao = \"nojo\"\n",
        "        elif emocao == \"neutral\":\n",
        "            emocao = \"neutro\"\n",
        "\n",
        "        # Detectamos caretas usando MediaPipe Face Mesh\n",
        "        # Se o FaceMesh encontrar pontos faciais, tentamos calcular se a pessoa está fazendo uma careta\n",
        "        if results.multi_face_landmarks:\n",
        "            for face_landmarks in results.multi_face_landmarks:\n",
        "\n",
        "                # Mapeamento de pontos da face\n",
        "                boca_esq = face_landmarks.landmark[61]\n",
        "                boca_dir = face_landmarks.landmark[291]\n",
        "                boca_labio_superior = face_landmarks.landmark[13]\n",
        "                boca_labio_inferior = face_landmarks.landmark[14]\n",
        "                olho_esq_superior = face_landmarks.landmark[159]\n",
        "                olho_esq_inferior = face_landmarks.landmark[145]\n",
        "                olho_dir_superior = face_landmarks.landmark[386]\n",
        "                olho_dir_inferior = face_landmarks.landmark[374]\n",
        "                face_testa = face_landmarks.landmark[10]\n",
        "                face_queixo = face_landmarks.landmark[152]\n",
        "\n",
        "                # Calcula os parâmetros necessários\n",
        "                altura_face = abs(face_queixo.y - face_testa.y) # tamanho vertical da face\n",
        "                abertura_boca = abs(boca_labio_inferior.y - boca_labio_superior.y) # tamanho da abertura da boca (unidades absolutas)\n",
        "                proporcao_boca_face = abertura_boca / altura_face # tamanho da abertura da boca em relação ao tamanho da face (proporção relativa)\n",
        "                abertura_olho_esq = abs(olho_esq_inferior.y - olho_esq_superior.y)\n",
        "                abertura_olho_dir = abs(olho_dir_inferior.y - olho_dir_superior.y)\n",
        "                assimetria_boca = abs(boca_esq.y - boca_dir.y) # assimetria da boca (unidades absolutas)\n",
        "\n",
        "                # Inicializamos o contador de confiança para a careta\n",
        "                confianca_careta = 0\n",
        "\n",
        "                # Avaliamos se os valores de expressões específicas são exagerados\n",
        "                # Arbitramos cada limite para detecção de caretas\n",
        "\n",
        "                # Se houver abertura expressiva da boca\n",
        "                if abertura_boca > 0.03:\n",
        "                    confianca_careta += 0.3\n",
        "                # Se houver abertura expressiva do olho esquerdo\n",
        "                if abertura_olho_esq > 0.025:\n",
        "                    confianca_careta += 0.15\n",
        "                # Se houver abertura expressiva do olho direito\n",
        "                if abertura_olho_dir > 0.025:\n",
        "                    confianca_careta += 0.15\n",
        "                # Se a proporção da boca em relação ao rosto indicar exagero\n",
        "                if proporcao_boca_face > 0.35:\n",
        "                    confianca_careta += 0.25\n",
        "                # Se houver uma assimetria consideravel na boca\n",
        "                if assimetria_boca >= 0.02:\n",
        "                    confianca_careta += 0.15\n",
        "\n",
        "                # Se confiança_careta for alta, mudamos a emoção registrada para \"careta\"\n",
        "                if confianca_careta >= 0.6:\n",
        "                    emocao = \"careta\"\n",
        "\n",
        "        # Incrementamos o contador da emoção detectada no dicionário `cont_emocoes`\n",
        "        if f\"cont_{emocao}\" in cont_emocoes:\n",
        "            cont_emocoes[f\"cont_{emocao}\"] += 1\n",
        "\n",
        "        # Caso a emoção seja \"careta\", o retângulo e o texto serão vermelhos, senão serão verdes.\n",
        "        cor = (0, 0, 255) if emocao == \"careta\" else (0, 255, 0)\n",
        "\n",
        "        # Desenhamos um retângulo ao redor do rosto detectado\n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), cor, 2)\n",
        "\n",
        "        # Escrevemos o nome da emoção detectada no frame\n",
        "        cv2.putText(\n",
        "            frame,\n",
        "            emocao,\n",
        "            (x + 2, y + 23),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.9,\n",
        "            cor,\n",
        "            2,\n",
        "        )\n",
        "\n",
        "        # Gravamos o histórico completo das emoções, número do frame e momento\n",
        "        historico_completo.append([cont_frames, tempo, emocao])\n",
        "\n",
        "        # Se necessário, grava o histórico parcial\n",
        "        if gravar_historico_parcial:\n",
        "            historico_parcial.append([cont_frames, tempo, emocao])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Função para encontrar o ponto médio da mão.  "
      ],
      "metadata": {
        "id": "DZNfofTp9GRL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YQvtL_Fn35S"
      },
      "source": [
        "\n",
        "A função mao tem como objetivo calcular a posição média dos pontos relevantes de uma mão (direita ou esquerda) com base nos dados fornecidos pelo modelo MediaPipe.\n",
        "Verificamos se os pontos estão ao mínimo visíveis em 50%, e retornamos a sua posição média. Caso não haja pontos válidos, a função retorna \"None\".\n",
        "\n",
        "1. Parâmetros da função:  \n",
        "    - landmarks: É a lista de pontos de referência (landmarks) da mão, como fornecido pelo modelo MediaPipe.\n",
        "    - direita: É um parâmetro booleano que determina se a mão a ser analisada é a direita (True) ou a esquerda (False). O valor padrão é True, assumindo que a mão a ser analisada é a direita.\n",
        "\n",
        "2. Inicialização das variáveis:  \n",
        "    - x, y: Variáveis para acumular as coordenadas médias dos pontos de interesse.\n",
        "    - c: Contador para o número de pontos válidos (visíveis) encontrados.\n",
        "\n",
        "3. Definição dos pontos de interesse:  \n",
        "O modelo MediaPipe usa índices específicos para representar os pontos da mão. O código define os pontos de interesse para a mão direita ou esquerda:\n",
        "    - Para a mão direita, são usados os pontos 16, 18, 20 e 22.\n",
        "    - Para a mão esquerda, são usados os pontos 15, 17, 19 e 21.\n",
        "\n",
        "4. Loop for:  \n",
        "    - Para cada ponto de interesse, a função verifica se ele está dentro de uma região válida, ou seja, se sua coordenada x, y está dentro do intervalo válido e se a visibilidade (visibility) do ponto é superior a 0.5 (ou seja, o ponto é suficientemente visível).\n",
        "    - Se o ponto for válido, a função incrementa o contador c e acumula as coordenadas x e y desses pontos.  \n",
        "\n",
        "5. Verificação de pontos válidos:  \n",
        "Caso não haja pontos válidos (c <= 0), a função retorna None, indicando que não foi possível calcular uma posição média.\n",
        "\n",
        "6. Cálculo das médias:  \n",
        "Após identificar os pontos válidos, a função calcula a média das coordenadas x e y desses pontos, dividindo a soma das coordenadas pelo número de pontos válidos (c).\n",
        "\n",
        "7. Retorno:  \n",
        "A função retorna a posição média calculada como uma lista de duas coordenadas [x, y]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dvdRPrPYfeD_"
      },
      "outputs": [],
      "source": [
        "# função para detecção de mão\n",
        "def mao(landmarks, direita = True):\n",
        "    x = 0  # Inicializa a variável para a coordenada x média\n",
        "    y = 0  # Inicializa a variável para a coordenada y média\n",
        "    c = 0  # Conta quantos pontos visíveis são considerados\n",
        "\n",
        "    # Definindo os pontos relevantes para a mão, dependendo se a mão é direita ou esquerda\n",
        "    pontos = [16, 18, 20, 22]       # Pontos da mão direita (baseados no modelo MediaPipe)\n",
        "    if not direita:\n",
        "        pontos = [15, 17, 19, 21]   # Pontos da mão esquerda (baseados no modelo MediaPipe)\n",
        "\n",
        "    # Loop para analisar os pontos definidos na lista\n",
        "    for val in pontos:\n",
        "        # Verifica se o ponto está dentro de uma região visível e válida\n",
        "        if landmarks[val].x > 0 and landmarks[val].y > 0 and landmarks[val].visibility > 0.5:\n",
        "            c += 1                  # Se o ponto for válido, incrementa o contador\n",
        "            x += landmarks[val].x   # Soma a coordenada x do ponto\n",
        "            y += landmarks[val].y   # Soma a coordenada y do ponto\n",
        "\n",
        "    # Se nenhum ponto válido for encontrado, retorna None\n",
        "    if c <= 0:\n",
        "        return None\n",
        "\n",
        "    # Calcula as médias das coordenadas x e y dos pontos válidos\n",
        "    x /= c  # Média da coordenada x\n",
        "    y /= c  # Média da coordenada y\n",
        "\n",
        "    # Retorna a posição média dos pontos como uma lista [x, y]\n",
        "    return [x, y]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Função para calcular a distância entre dois pontos"
      ],
      "metadata": {
        "id": "dOuxziM_AGM9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjQZRSZQS3mV"
      },
      "source": [
        "Esta função apura a distância euclidiana entre dois pontos.  \n",
        "Se ambos os pontos forem válidos, a função calcula a distância Euclidiana entre eles. A fórmula utilizada é:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "Onde (x1, y1) e (x2, y2) são as coordenadas dos pontos p1 e p2, respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Thz6KYRZzbnE"
      },
      "outputs": [],
      "source": [
        "# função para calcular a distância entre dois pontos\n",
        "def distancia(p1, p2):\n",
        "    # Verifica se algum dos pontos fornecidos é None, retornando None em caso afirmativo\n",
        "    if p1 is None or p2 is None:\n",
        "        return None\n",
        "\n",
        "    # Calcula a distância Euclidiana entre os pontos p1 e p2\n",
        "    return np.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Função para detecção da posição da mão em relação a cabeça e rosto."
      ],
      "metadata": {
        "id": "vdxLMdl0ALxc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoFOpddIwEwV"
      },
      "source": [
        "1. Contagem de atividades: Dois contadores são usados para rastrear atividades específicas: \"mão levantada\" e \"mão no rosto\".\n",
        "\n",
        "2. Detecção de mãos e rosto: A função usa a biblioteca mediapipe para detectar marcos de pose no frame, identificando as posições das mãos e do rosto.\n",
        "\n",
        "3. Processamento do rosto: Para cada rosto, calcula-se o centro e a maior distância entre dois pontos, além da posição das mãos.\n",
        "\n",
        "4. Condicional de atividades:\n",
        "    - Se a mão estiver acima da posição mais alta do rosto, é identificado como \"mão levantada\".\n",
        "    - Se a distância entre a mão e o centro do rosto for pequena o suficiente, é identificado como \"mão no rosto\".\n",
        "\n",
        "5. Exibição no frame: Usa cv2.putText() para exibir as atividades detectadas (\"mão levantada\" ou \"mão no rosto\") no frame.\n",
        "\n",
        "6. Registro de atividades: Além da exibição no frame, o código mantém um histórico das atividades detectadas e atualiza os contadores correspondentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2UJLM-jSYfap"
      },
      "outputs": [],
      "source": [
        "# contadores de atividades\n",
        "cont_atividades = {\n",
        "    \"cont_mao_levantada\": 0,    # Contador para \"mão levantada\"\n",
        "    \"cont_mao_rosto\"    : 0     # Contador para \"mão no rosto\"\n",
        "}\n",
        "\n",
        "# função para detecção de mãos em relação ao rosto\n",
        "def detecta_mao_x_rosto(frame):\n",
        "    # Converte o frame de BGR (formato OpenCV padrão) para RGB\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Processa o frame de imagem para ser compatível com a detecção do modelo\n",
        "    mp_image = mp.Image(\n",
        "        image_format=mp.ImageFormat.SRGB,\n",
        "        data=rgb_frame\n",
        "    )\n",
        "\n",
        "    # Realiza a detecção dos marcos (landmarks) do corpo\n",
        "    results = landmarker.detect(mp_image)\n",
        "\n",
        "    # Se forem encontrados marcos de pose (como a posição dos membros), trata a posição das mãos em relação ao rosto\n",
        "    if results.pose_landmarks:\n",
        "        for pose_landmarks in results.pose_landmarks:\n",
        "            # e coleta as posições das mãos, tanto direita quanto esquerda\n",
        "            mao_direita = mao(pose_landmarks, True)\n",
        "            mao_esquerda = mao(pose_landmarks, False)\n",
        "\n",
        "            # Se não for detectada nenhuma mão, retorna sem processamento\n",
        "            if mao_direita is None and mao_esquerda is None:\n",
        "                return\n",
        "\n",
        "           # Variáveis para calcular o centro do rosto, a maior distância e o ponto mais alto do rosto\n",
        "            x_rosto      = 0\n",
        "            y_rosto      = 0\n",
        "            min_y_rosto  = 0\n",
        "            max_dist     = 0  # A maior distância entre dois pontos no rosto vai servir como escala de tamanho\n",
        "            cont_p_rosto = 0\n",
        "\n",
        "            # Processa os marcos da cabeça (de 0 a 10) para encontrar o centro e a maior distância\n",
        "            for i in range(0, 11):\n",
        "                if pose_landmarks[i].visibility > 0.5 and pose_landmarks[i].x > 0 and pose_landmarks[i].y > 0:\n",
        "                    x_rosto += pose_landmarks[i].x\n",
        "                    y_rosto += pose_landmarks[i].y\n",
        "                    cont_p_rosto += 1\n",
        "\n",
        "                    if min_y_rosto == 0 or pose_landmarks[i].y < min_y_rosto:\n",
        "                        min_y_rosto = pose_landmarks[i].y\n",
        "\n",
        "                    for j in range(i + 1, 11):\n",
        "                        if pose_landmarks[j].visibility > 0.5 and pose_landmarks[j].x > 0 and pose_landmarks[j].y > 0:\n",
        "                            p1 = [pose_landmarks[i].x, pose_landmarks[i].y]\n",
        "                            p2 = [pose_landmarks[j].x, pose_landmarks[j].y]\n",
        "                            dist = distancia(p1, p2)\n",
        "\n",
        "                            if dist is not None and dist > max_dist:\n",
        "                                max_dist = dist\n",
        "\n",
        "            # Se não houver elementos da cabeça válidos, retorna sem processamento\n",
        "            if cont_p_rosto == 0:\n",
        "                return\n",
        "\n",
        "            # Calcula o centro do rosto a partir da média dos marcos\n",
        "            x_rosto = x_rosto / cont_p_rosto\n",
        "            y_rosto = y_rosto / cont_p_rosto\n",
        "\n",
        "            centro_rosto = [x_rosto, y_rosto]\n",
        "\n",
        "            # Identifica e coleta a mão mais alta, menor y positivo de uma das mãos\n",
        "            x_mao_mais_alta = 0\n",
        "            y_mao_mais_alta = 0\n",
        "            if mao_direita is not None and mao_direita[1] > 0:\n",
        "                x_mao_mais_alta = mao_direita[0]\n",
        "                y_mao_mais_alta = mao_direita[1]\n",
        "                if mao_esquerda is not None and  mao_esquerda[1] > 0 and mao_esquerda[1] < y_mao_mais_alta:\n",
        "                    x_mao_mais_alta = mao_esquerda[0]\n",
        "                    y_mao_mais_alta = mao_esquerda[1]\n",
        "            elif mao_esquerda is not None and mao_esquerda[1] > 0:\n",
        "                x_mao_mais_alta = mao_esquerda[0]\n",
        "                y_mao_mais_alta = mao_esquerda[1]\n",
        "                if mao_direita is not None and mao_direita[1] > 0 and mao_direita[1] < y_mao_mais_alta:\n",
        "                    x_mao_mais_alta = mao_direita[0]\n",
        "                    y_mao_mais_alta = mao_direita[1]\n",
        "\n",
        "            # Se a mão estiver acima do rosto, o seu y sera menor que o menor y de elemento do rosto\n",
        "            if y_mao_mais_alta != 0 and y_mao_mais_alta < min_y_rosto:\n",
        "                cv2.putText(\n",
        "                    frame,\n",
        "                    \"MAO LEVANTADA\",    # escreve 'mão levantada' no frame\n",
        "                    (int(width * x_mao_mais_alta), int(height * y_mao_mais_alta)),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.9,\n",
        "                    (0, 0, 255),\n",
        "                    2,\n",
        "                )\n",
        "                cont_atividades[\"cont_mao_levantada\"] += 1\n",
        "                historico_completo.append([cont_frames, tempo, \"mao levantada\"])\n",
        "\n",
        "                if gravar_historico_parcial:\n",
        "                    historico_parcial.append([cont_frames, tempo, \"mao levantada\"])\n",
        "\n",
        "            # se a distância de qualquer uma das mãos até o centro do rosto for maior do que o max_dist\n",
        "            # escreve 'mão no rosto' no frame\n",
        "            if max_dist > 0:\n",
        "                max_dist_frame = max(width, height) # valor máximo no frame\n",
        "\n",
        "                # apura a distância da mão até o centro do rosto\n",
        "                dist_mao_direita_rosto = max_dist_frame\n",
        "                if mao_direita is not None:\n",
        "                    dist_mao_direita_rosto = distancia(mao_direita, centro_rosto)\n",
        "                dist_mao_esquerda_rosto = max_dist_frame\n",
        "\n",
        "                if mao_esquerda is not None:\n",
        "                    dist_mao_esquerda_rosto = distancia(mao_esquerda, centro_rosto)\n",
        "\n",
        "                # Se a distância for dentro de um limite, escreve \"MÃO NO ROSTO\" no frame\n",
        "                if dist_mao_direita_rosto is not None and dist_mao_direita_rosto <= 1.8 * max_dist:\n",
        "                    cv2.putText(\n",
        "                        frame,\n",
        "                        \"MAO NO ROSTO\",\n",
        "                        (int(width * mao_direita[0]), int(height * mao_direita[1])),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                        0.9,\n",
        "                        (0, 0, 255),\n",
        "                        2,\n",
        "                    )\n",
        "                    cont_atividades[\"cont_mao_rosto\"] += 1\n",
        "                    historico_completo.append([cont_frames, tempo, \"mao no rosto\"])\n",
        "\n",
        "                    if gravar_historico_parcial:\n",
        "                        historico_parcial.append([cont_frames, tempo, \"mao no rosto\"])\n",
        "\n",
        "                if dist_mao_esquerda_rosto is not None and dist_mao_esquerda_rosto <= 1.8 * max_dist:\n",
        "                    cv2.putText(\n",
        "                        frame,\n",
        "                        \"MAO NO ROSTO\",\n",
        "                        (int(width * mao_esquerda[0]), int(height * mao_esquerda[1])),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                        0.9,\n",
        "                        (0, 0, 255),\n",
        "                        2,\n",
        "                    )\n",
        "                    cont_atividades[\"cont_mao_rosto\"] += 1\n",
        "                    historico_completo.append([cont_frames, tempo, \"mao no rosto\"])\n",
        "\n",
        "                    if gravar_historico_parcial:\n",
        "                        historico_parcial.append([cont_frames, tempo, \"mao no rosto\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loop para processamento do vídeo frame a frame."
      ],
      "metadata": {
        "id": "FxzgwHGJAUB3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVRRaXzhzJzW"
      },
      "source": [
        "1. Contagem de frames (cont_frames):  \n",
        "Inicializa o contador de frames com 0. Para cada iteração do loop, incrementa este contador.\n",
        "\n",
        "2. Leitura de frames:  \n",
        "Usa cap.read() para ler o próximo frame do vídeo. Caso não seja possível (fim do vídeo ou erro), o loop é interrompido.\n",
        "\n",
        "3. Gravação condicional do histórico parcial:  \n",
        "O histórico parcial é gravado apenas em frames específicos: no meio do vídeo (fps//2) e depois a cada intervalo de fps frames a partir desse ponto.\n",
        "\n",
        "4. Conversão de frames para tempo (tempo):  \n",
        "Converte o número total de frames processados até o momento em tempo no formato MM:SS.\n",
        "\n",
        "5. Funções de detecção:  \n",
        "detecta_rostos(frame): Detecta rostos no frame atual.\n",
        "detecta_mao_x_rosto(frame): Detecta a posição das mãos em relação ao rosto no frame atual.\n",
        "\n",
        "6. Gravação do frame processado:  \n",
        "Após processar o frame (detecção de rostos e mãos), o frame é gravado no arquivo de saída (out.write(frame))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6Q3p65DZY58",
        "outputId": "c52b4656-8f3c-4907-cc60-5418bf57f72b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessando vídeo:   0%|          | 0/3326 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24-11-24 13:57:09 - Downloading Yolo weights from https://drive.google.com/uc?id=1qcr9DbgsX3ryrz2uU8w4Xm3cOrRywXqb to /root/.deepface/weights/yolov8n-face.pt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qcr9DbgsX3ryrz2uU8w4Xm3cOrRywXqb\n",
            "To: /root/.deepface/weights/yolov8n-face.pt\n",
            "\n",
            "  0%|          | 0.00/6.39M [00:00<?, ?B/s]\u001b[A\n",
            "100%|██████████| 6.39M/6.39M [00:00<00:00, 49.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24-11-24 13:57:13 - Yolo model is just downloaded to yolov8n-face.pt\n",
            "24-11-24 13:57:20 - facial_expression_model_weights.h5 will be downloaded...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/facial_expression_model_weights.h5\n",
            "To: /root/.deepface/weights/facial_expression_model_weights.h5\n",
            "\n",
            "  0%|          | 0.00/5.98M [00:00<?, ?B/s]\u001b[A\n",
            "100%|██████████| 5.98M/5.98M [00:00<00:00, 50.6MB/s]\n",
            "Processando vídeo: 100%|██████████| 3326/3326 [46:24<00:00,  1.19it/s]\n"
          ]
        }
      ],
      "source": [
        "# Contador de frames\n",
        "cont_frames = 0\n",
        "\n",
        "# Loop para processaro video frame a frame\n",
        "for _ in tqdm(range(total_frames), desc=\"Processando vídeo\", total=total_frames):\n",
        "    ret, frame = cap.read()  # Lê o frame do vídeo\n",
        "\n",
        "    # Se não for possível ler o frame (ex: fim do vídeo), interrompe o loop\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # A contagem de frames = (tempo no vídeo em segundos) x fps\n",
        "    cont_frames += 1\n",
        "\n",
        "    # Processar apenas uma parte do vídeo se necessário\n",
        "    # if cont_frames < (0 * fps):\n",
        "    #     continue\n",
        "    # if cont_frames > (10 * fps):\n",
        "    #     break\n",
        "\n",
        "    # Nós sempre gravamos a atividade no histórico completo\n",
        "    # E tambem gravaremos a atividade no histórico parcial se o frame for o mediano (nesse caso 15) ou a cada número de \"fps\" (nesse caso 30) depois do mediano\n",
        "    if cont_frames == (fps//2) or (cont_frames - (fps//2)) % fps == 0:\n",
        "        gravar_historico_parcial = True     # Marca para gravar no histórico parcial\n",
        "    else:\n",
        "        gravar_historico_parcial = False    # Marca para não gravar no histórico parcial\n",
        "\n",
        "\n",
        "    # Tempo em formato MM:SS\n",
        "    tempo = f\"{int(cont_frames / fps // 60):02d}:{int(cont_frames / fps % 60):02d}\"\n",
        "\n",
        "    # Detecta rostos no frame\n",
        "    detecta_rostos(frame)\n",
        "\n",
        "    # Detecta posição de mãos em relação ao rostos\n",
        "    detecta_mao_x_rosto(frame)\n",
        "\n",
        "    # Escreve o frame no arquivo de saída\n",
        "    out.write(frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparação de outputs"
      ],
      "metadata": {
        "id": "PC1TqbkyApt6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rCA3RBmJEm7A"
      },
      "outputs": [],
      "source": [
        "# Libera recursos\n",
        "cap.release()\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpieAJTLQGeO"
      },
      "source": [
        "Nesse bloco, criamos dois arquivos csv para gerar os arquivos de histórico parcial e completo utilizando nossas variáveis de histórico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3gdnwuEXD7OZ"
      },
      "outputs": [],
      "source": [
        "# Cria um arquivo csv para registrar o histórico parcial\n",
        "with open(hist_parcial_path, \"w\") as f1:\n",
        "    writer1 = csv.writer(f1)\n",
        "\n",
        "    # Adiciona um linha de cabeçado com as colunas de dados\n",
        "    writer1.writerow([\"frame\", \"tempo\", \"atividade/emocao\"])\n",
        "\n",
        "    # Percorre o histórico de atividades para adicionar linhas ao histórico\n",
        "    for row in historico_parcial:\n",
        "        writer1.writerow(row)\n",
        "\n",
        "# Cria um arquivo csv para registrar o histórico completo\n",
        "with open(hist_completo_path, \"w\") as f2:\n",
        "    writer2 = csv.writer(f2)\n",
        "\n",
        "    # Adiciona um linha de cabeçado com as colunas de dados\n",
        "    writer2.writerow([\"frame\", \"tempo\", \"atividade/emocao\"])\n",
        "\n",
        "    # Percorre o histórico de atividades para adicionar linhas ao histórico\n",
        "    for row in historico_completo:\n",
        "        writer2.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criação do resumo detalhado das emoções e atividades detectadas no vídeo.  "
      ],
      "metadata": {
        "id": "FDw2GchBBHK1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyfzBxbD1OUT"
      },
      "source": [
        "1. Inicialização das variáveis:  \n",
        "    - Um dicionário porcentagem_emocoes é criado para armazenar a porcentagem de cada emoção detectada no vídeo.\n",
        "    - A soma total dos frames de emoções e atividades é calculada.\n",
        "\n",
        "2. Cálculo das porcentagens:\n",
        "    - Para cada tipo de emoção, a porcentagem é calculada com base no número de frames em que a emoção foi detectada, em relação ao total de frames com emoções detectadas.\n",
        "\n",
        "3. Criação de tabelas:\n",
        "    - São criadas duas tabelas: uma para as emoções detectadas, contendo o nome da emoção, a quantidade de frames e a porcentagem, e outra para as atividades (como \"Mão levantada\" e \"Mão no rosto\"), contendo o nome da atividade e a quantidade de frames.\n",
        "\n",
        "4. Geração do relatório:\n",
        "    - Um arquivo de texto (relat_path) é gerado para registrar um relatório detalhado. O relatório inclui:\n",
        "        - Total de frames analisados.\n",
        "        - Número de frames com anomalias (caretas).\n",
        "        - Tabelas com as emoções e atividades detectadas, incluindo a contagem de frames e as porcentagens.\n",
        "5. Comentario Relevante:\n",
        "    - Se múltiplas emoções ou atividades forem detectadas em um único frame, o número de frames para essas emoções/atividades pode ser maior que o total de frames do vídeo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "b7_g_q2SrlHq"
      },
      "outputs": [],
      "source": [
        "# Armazenagem da porcentagem que cada emoção representa do total de emoções detectadas no vídeo\n",
        "porcentagem_emocoes = {\n",
        "    \"porc_triste\": 0,\n",
        "    \"porc_bravo\": 0,\n",
        "    \"porc_surpreso\": 0,\n",
        "    \"porc_medo\": 0,\n",
        "    \"porc_feliz\": 0,\n",
        "    \"porc_nojo\": 0,\n",
        "    \"porc_neutro\": 0,\n",
        "    \"porc_careta\": 0\n",
        "    }\n",
        "\n",
        "# Soma total de frames de emoções\n",
        "total_emocoes = sum(cont_emocoes.values())\n",
        "\n",
        "# Soma total de frames de atividades\n",
        "total_atividades = sum(cont_atividades.values())\n",
        "\n",
        "# Calcula a porcentagem de cada contador\n",
        "for contador, valor in cont_emocoes.items():\n",
        "    porcentagem_emocoes[f\"porc_{contador.split('_')[1]}\"] = (valor / total_emocoes) * 100 if total_emocoes > 0 else 0\n",
        "\n",
        "\n",
        "# Criando tabelas\n",
        "table_emocoes = PrettyTable()\n",
        "table_emocoes.field_names = [\"Emoção\", \"Frames\", \"Porcentagem\"]\n",
        "table_emocoes.add_row([\"Triste\", cont_emocoes[\"cont_triste\"], f\"{porcentagem_emocoes['porc_triste']:.1f}%\"])\n",
        "table_emocoes.add_row([\"Bravo\", cont_emocoes[\"cont_bravo\"], f\"{porcentagem_emocoes['porc_bravo']:.1f}%\"])\n",
        "table_emocoes.add_row([\"Surpreso\", cont_emocoes[\"cont_surpreso\"], f\"{porcentagem_emocoes['porc_surpreso']:.1f}%\"])\n",
        "table_emocoes.add_row([\"Medo\", cont_emocoes[\"cont_medo\"], f\"{porcentagem_emocoes['porc_medo']:.1f}%\"])\n",
        "table_emocoes.add_row([\"Feliz\", cont_emocoes[\"cont_feliz\"], f\"{porcentagem_emocoes['porc_feliz']:.1f}%\"])\n",
        "table_emocoes.add_row([\"Nojo\", cont_emocoes[\"cont_nojo\"], f\"{porcentagem_emocoes['porc_nojo']:.1f}%\"])\n",
        "table_emocoes.add_row([\"Neutro\", cont_emocoes[\"cont_neutro\"], f\"{porcentagem_emocoes['porc_neutro']:.1f}%\"])\n",
        "table_emocoes.add_row([\"Careta\", cont_emocoes[\"cont_careta\"], f\"{porcentagem_emocoes['porc_careta']:.1f}%\"])\n",
        "\n",
        "table_atividades = PrettyTable()\n",
        "table_atividades.field_names = [\"Atividade\", \"Frames\"]\n",
        "table_atividades.add_row([\"Mão levantada\", cont_atividades[\"cont_mao_levantada\"]])\n",
        "table_atividades.add_row([\"Mão no rosto\", cont_atividades[\"cont_mao_rosto\"]])\n",
        "\n",
        "# Cria um arquivo txt para registrar o relatório\n",
        "with open(relat_path, \"w\") as f3:\n",
        "    f3.write(\"=\"*30 + \"\\n\")\n",
        "    f3.write(\"Relatório de Análise de Vídeo\\n\")\n",
        "    f3.write(\"=\"*30 + \"\\n\\n\")\n",
        "    f3.write(f\"Total de frames analisados: {cont_frames}\\n\")\n",
        "    f3.write(f\"Número de frames contendo anomalias (caretas): {cont_emocoes['cont_careta']}\\n\\n\")\n",
        "    f3.write(\"Emoções Detectadas\\n\")\n",
        "    f3.write(table_emocoes.get_string())  # Escreve a tabela formatada no arquivo\n",
        "    f3.write(\"\\n\")\n",
        "    f3.write(f\"Total de frames de emoções: {total_emocoes}\")\n",
        "    f3.write(\"\\n\\n\")\n",
        "    f3.write(\"Contagem de Atividades\\n\")\n",
        "    f3.write(table_atividades.get_string())  # Escreve a tabela formatada no arquivo\n",
        "    f3.write(\"\\n\")\n",
        "    f3.write(f\"Total de frames de atividades: {total_atividades}\")\n",
        "    f3.write(\"\\n\\n\")\n",
        "    f3.write(\"Obs.: Os Frames nas tabelas se referem a quantidade de frames que aquela emoção/atividade foi detectada.\"\n",
        "    + \"\\nLogo, se mais de uma emoção (em múltiplos rostos) ou atividade foi detectada em um frame, o total de frames de emoção/atividade aumentará em correspondência a isso.\"\n",
        "    + \"\\nNão seria estranho haver mais frames de emoções/atividades do que frames totais no vídeo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivqenaUgOEXZ"
      },
      "source": [
        "## CONCLUSÕES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc1fX9UC22Lr"
      },
      "source": [
        "A nossa proposta de código para o Tech Challenge 4 é útil para análise de emoções e atividades não só no video proposto pela FIAP, mas também em qualquer vídeo exposto, utilizando técnicas de processamento de imagem para detectar emoções faciais e interações com movimentos das mãos.  \n",
        "\n",
        "Ele pode ser útil em várias aplicações, incluindo:  \n",
        "\n",
        "- Análise de Emoções: O código permite identificar e quantificar a presença de emoções como tristeza, raiva, surpresa, medo, felicidade, nojo, neutro e caretas em vídeos, proporcionando uma visão geral do comportamento emocional de indivíduos ao longo do tempo.\n",
        "\n",
        "- Monitoramento de Atividades: Ele também permite detectar atividades específicas, como mãos levantadas e mãos no rosto a cada frame do vídeo. Esse tipo de análise pode ser aplicado em cenários como estudo de interações em ambientes corporativos, salas de aula ou até mesmo em vídeos de reuniões.\n",
        "\n",
        "- Geração de Relatórios: O código gera relatórios detalhados sobre a análise do vídeo, registrando a quantidade de frames de emoções e atividades detectadas, bem como suas respectivas porcentagens. Isso é útil tanto para criar uma visão quantitativa das emoções e atividades ao longo do vídeo quanto para fornecer um histórico completo de como o vídeo foi analisado.\n",
        "\n",
        "- Aplicações em Áreas Diversas: Pode ser usado em pesquisas acadêmicas sobre comportamento humano, sistemas de reconhecimento facial para segurança, análises de vídeos de treinamentos, educação, ou mesmo em marketing, para entender como as pessoas reagem emocionalmente a anúncios ou produtos.\n",
        "\n",
        "Vale observar que, como qualquer sistema de processamento de imagem, ele apresenta algumas limitações e desafios operacionais. A seguir, destacam-se considerações importantes relacionadas à precisão, escalabilidade, flexibilidade e interatividade do código.\n",
        "\n",
        "- Precisão e Limitações: Embora o código seja eficaz na detecção de emoções e atividades, a precisão do modelo pode ser afetada por vários fatores, como qualidade da imagem, iluminação, ângulos de captura e a diversidade de expressões faciais e movimentos de mãos. Para garantir melhores resultados, a qualidade do vídeo de entrada deve ser boa e as emoções ou atividades devem ser claramente visíveis no quadro.\n",
        "\n",
        "- Escalabilidade: O código está estruturado para processar vídeos frame a frame. Dependendo da duração e do número de frames do vídeo, isso pode ser uma operação intensiva em termos de tempo de processamento, especialmente se o número de vídeos a serem analisados for grande.\n",
        "\n",
        "- Flexibilidade: O código pode ser facilmente adaptado para detectar outras emoções ou atividades além das que já estão previstas, bastando adicionar novos detectores ou ajustes no processamento das imagens.\n",
        "\n",
        "- Interatividade: Para análise em tempo real, é possível integrar esse código a sistemas que permitam a detecção contínua de emoções e atividades, como monitoramento ao vivo de reuniões ou eventos."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "vHuEOWXtM0Ru",
        "N12udYXdGh76",
        "JdB2vpxhNAp8",
        "PARvUghINLsl",
        "yMHrMBGnNSUP",
        "3DS5FzysNvrw",
        "oFSllwzsuY45",
        "ie94Cd9nupgy",
        "8UoSGd-kB9Py",
        "5aIKh2vuBph9",
        "-jo69XOFClOW",
        "8jd2RV-FEPRV",
        "OLRS6Ut_EXwB",
        "STpvkJ1KFRlR",
        "DZNfofTp9GRL",
        "dOuxziM_AGM9",
        "vdxLMdl0ALxc",
        "FxzgwHGJAUB3",
        "PC1TqbkyApt6",
        "FDw2GchBBHK1",
        "ivqenaUgOEXZ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}